# RandomForest-Bagging-Boosting
Comparing tree based methods with those of forest based methods for regression. Tree-based methods are simple and useful for interpretation. However, they typically are not competitive with the best supervised learning approaches. Hence I introduce bagging, random forests, and boosting. Each of these approaches involves producing multiple trees which are then combined to yield a single consensus prediction. We will see that combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation.
