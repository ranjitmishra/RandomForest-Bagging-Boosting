---
title: "EnsembleMethods"
author: "Ranjit Mishra"
date: "June 28, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

## Index

- Load required packages
- Simple Classification
- Cross-Validation
- Random Forest
- Boosting



## Random Forests & Bagging

Used for trees as building blocks to build more complex models. Random forests build lots of bushy trees, and then average them to reduce the variance. Bagging uses all variables and is same as random forest in terms of mathemetics. Here we will use the Bosotn data to explore random forests and boosting. Lets start with a simple tree and compare its result with forest based trees

```{r}
boston <- read.csv("~/Analytics/Boston.csv", sep = ",", header = TRUE)
str(boston)

summary(boston)
hist(boston$medv, col = 'cyan')
```

## Regression Tree

Here we fit a regression tree to the Boston data set. First, we create a training set, and fit the tree to the training data

```{r}
set.seed (21)
library(tree)

train = sample (1: nrow(boston), nrow(boston)/2)

tree.boston = tree(medv ~., boston ,subset =train)

tree.boston
```

##Summary of the tree

Notice that the output of summary() indicates that only five of the variables have been used in constructing the tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree

```{r}
summary(tree.boston)
```

##Plot the tree

```{r}
plot(tree.boston)
text(tree.boston, pretty= 0)
```

##Use the Tree for Prediction

We are not using pruning of the tree for this demonstration. Lets use the unpruned tree to
make predictions on the test set.

```{r}
set.seed(21)
yhat <- predict(tree.boston, newdata = boston[-train ,])

yhat.test <- boston[-train, "medv"]
mse = mean((yhat - yhat.test)**2)
cat('MSE from regression tree is: ',mse )
```

## Bagging

Bagging is simply a special case of a random forest with m = p. Therefore, the randomForest() function can be used to perform both random forests and bagging. Here, mtry should be equal to the total number of independent variables

```{r}
library(randomForest)

set.seed(21)

bag.boston = randomForest(medv ~ ., data=boston,         subset=train, mtry= 8, importance =TRUE)

bag.boston

yhat.bag = predict (bag.boston, newdata = boston[-train ,])

cat('MSE from bagging is: ', (mean(( yhat.bag - yhat.test)^2)))
```

## Bagging - Plots

```{r}
plot(yhat.bag , yhat.test)
abline (0,1)
```

## Bagging ImpVar

Two measures of variable importance are reported. The former is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is excluded from the model. The latter is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees. In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. Plots of these importance measures can also be produced 

```{r}
importance (bag.boston)
varImpPlot (bag.boston, col = 'red' )
```

## Fit RF Model

Lets fit a random forest and see how well it performs. We will use the response variable medv. Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument. By default, randomForest() uses p/3 variables when building a random forest of regression trees, and sqrt(p) variables when building a random forest of classification trees. Here we use mtry = 3  

```{r}
set.seed(21)
rf.boston = randomForest(medv ~ . , data = boston,
                subset = train, mtry= 3, importance =TRUE)

rf.boston

yhat.rf = predict (rf.boston ,newdata = boston[-train, ])

mean(( yhat.rf - yhat.test)^2)
cat('MSE from random forest is: ', (mean(( yhat.bag - yhat.test)^2)))
```

## Plot Variable Importance

Using the importance() function, we can view the importance of each variable.Plots of these importance measures can be produced using the varImpPlot() function

```{r}
importance (rf.boston )
varImpPlot (rf.boston, col= 'blue')
```

## Error Estimate

The MSR and % variance explained are based on OOB or out-of-bag estimates, a very clever device in random forests to get honest error estimates. The model reports that mtry=3, which is the number of variables randomly chosen at each split. Since ( p=8) here, we could try all 8 possible values of mtry. We will do so, record the results, and make a plot.

```{r}
set.seed(21)
oob.err = double(8)
test.err = double(8)
for (mtry in 1:8) {
    fit = randomForest(medv ~ ., data = boston, subset = train, mtry = mtry, ntree = 400)
    oob.err[mtry] = fit$mse[400]
    pred = predict(fit, boston[-train, ])
    test.err[mtry] = with(boston[-train, ], mean((medv - pred)^2))
    cat(mtry, " ")
}
```

## Plot Error

```{r}
matplot(1:mtry, cbind(test.err, oob.err), pch = 19, col = c("red", "blue"), 
    type = "b", ylab = "Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 19, col = c("red", "blue"))
```

The test-error curve and OOB curve, these are estimates based on data, and so have their own standard errors (which are typically quite large). 

## Boosting

Boosting builds lots of smaller trees. Unlike random forests, each new tree in boosting tries to patch up the deficiencies of the current ensemble. Here we use the gbm function 

```{r}
library(gbm)

set.seed(21)

boost.boston = gbm(medv ~. , data = boston[train, ], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)

summary(boost.boston)
```

## Boosting Prediction

Lets make a prediction on the test set. With boosting, the number of trees is a tuning parameter, and if we have too many we can overfit. So we should use cross-validation to select the number of trees. We will leave this as an exercise. Instead, we will compute the test error as a function of the number of trees, and make a plot.

```{r}
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = boston[-train, ], n.trees = n.trees)
dim(predmat)

cat('MSE from boosting is: ', (mean(( yhat.bag - yhat.test)^2)))
```
